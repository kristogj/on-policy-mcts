game_config:
  # What game should be played
  game_type: "hex"

  hex:
    # Size of Hex board where 3 <= size <= 10
    board_size: 3

  # Turn on/off game statistic/prints during games
  verbose: 0

mcts_config:
  # Number of episodes (actual games played) in MCTS
  episodes: 200

  # Number of simulations done in MCTS before every actual action in the game
  num_sim: 500

  # Exploration constant
  c: 1

  # Which player should start
  starting_player: 1

  # Turn on/off visualizer
  visualize: True

  # Visualize every X epoch
  visualize_interval: 50

anet_config:
  # Learning rate for the neural net used by the actor
  lr: 0.01

  # Layer specs
  layer_specs: [10, 64, 9]

  # Activation function for each layer e.g linear, sigmoid, tanh, relu
  layer_func: ["tanh", "linear"]

  # Optimizer used for training actor e.g adagrad, sgd, rmsprop, adam
  optim: "adam"

  # Mini-batch size used when sampling from ReplayBuffer
  batch_size: 64

  # Probability of not using as actor
  epsilon_actor: 1

  # Decay rate epsilon actor
  dr_epsilon_actor: 0.99

  # Probability of not using critic
  epsilon_critic: 2 # Do not want to use critic in the start

  # Decay rate epsilon critic
  dr_epsilon_critic: 0.995


topp_config:
  # Number of times to save the current state of the actor during training.
  # For example, if you are training for a total of 200 episodes with M=5, you will have ANETs trained for 0, 50, 100,
  # 150 and 200 episodes.
  M: 5

  # Number of games to be played between any two ANET-based agents that meet during the round-robin
  # play of the TOPP
  G: 100

  # Folder to load all agents from
  agent_path: "./pretrained"




